{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_CLASSES = 3 # number of classes in the csv\n",
    "\n",
    "max_seq_length = 1024*3 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "\n",
    "model_name = \"unsloth/Qwen3-4B-Base\"; load_in_4bit = False\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,load_in_4bit = load_in_4bit,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    gpu_memory_utilization = 0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d451624",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_token_ids = []\n",
    "for i in range(0, NUM_CLASSES):\n",
    "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
    "# keep only the number tokens from lm_head\n",
    "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
    "\n",
    "old_shape = model.lm_head.weight.shape\n",
    "old_size = old_shape[0]\n",
    "print(par.shape)\n",
    "print(old_shape)\n",
    "\n",
    "model.lm_head.weight = par\n",
    "\n",
    "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
    "reverse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9793a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoftQConfig\n",
    "\n",
    "lora_rank = 16\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"lm_head\", \"embed_tokens\" # can easily be trained because it now has a small size\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    # init_lora_weights = 'loftq',\n",
    "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
    ")\n",
    "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8474067",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"### INSTRUCTION ###\n",
    "You are a fact-checking system.  \n",
    "Classify the \"Response\" to the \"Question\" into one of three classes:  \n",
    "- class 0: no → Correct answer, consistent with context, no extra info.  \n",
    "- class 1: intrinsic → Contradicts or distorts context info.  \n",
    "- class 2: extrinsic → Adds info not in context.  \n",
    "### INPUT ###\n",
    "Context: {context}  \n",
    "Response: {response}  \n",
    "### OUTPUT ###\n",
    "The correct answer is: class {label}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cc794",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'no': '0',\n",
    "    'intrinsic': '1',\n",
    "    'extrinsic': '2'\n",
    "}\n",
    "label2id = {\"no\": 0, \"intrinsic\": 1, \"extrinsic\": 2}\n",
    "\n",
    "def formatting_prompts_func(dataset_, prompt_template):\n",
    "    texts = []\n",
    "    for index, row in dataset_.iterrows():\n",
    "        # Lấy dữ liệu và strip() để loại bỏ khoảng trắng thừa\n",
    "        context_ = str(row.get('context', '')).strip()\n",
    "        question_ = str(row.get('prompt', '')).strip()\n",
    "        response_ = str(row.get('response', '')).strip()\n",
    "        \n",
    "        # Lấy nhãn chữ và loại bỏ khoảng trắng thừa\n",
    "        label_text = str(row.get('label', '')).strip()\n",
    "        \n",
    "        # *** DÒNG QUAN TRỌNG NHẤT: Ánh xạ nhãn chữ sang nhãn số ***\n",
    "        label_ = label_map.get(label_text, '') # Dùng .get để tránh lỗi nếu có nhãn lạ\n",
    "        \n",
    "        # Điền thông tin vào khuôn mẫu\n",
    "        text = prompt_template.format(\n",
    "            context=context_, \n",
    "            # question=question_, \n",
    "            response=response_, \n",
    "            label=label_\n",
    "        )\n",
    "        \n",
    "        texts.append(text)\n",
    "        \n",
    "    return texts\n",
    "\n",
    "def encode_label(example):\n",
    "    example[\"label\"] = label2id[example[\"label\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df_original = pd.read_csv(\"/home/tamanh/dsc2025/dsc2025/data/vihallu-train.csv\")\n",
    "train_df, eval_df = train_test_split(train_df_original, test_size=0.1, random_state=36)\n",
    "\n",
    "train_df[\"text\"] = formatting_prompts_func(train_df, prompt)\n",
    "eval_df[\"text\"] = formatting_prompts_func(eval_df, prompt)\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df, preserve_index=True)\n",
    "eval_dataset = datasets.Dataset.from_pandas(eval_df, preserve_index=True)\n",
    "train_dataset = train_dataset.map(encode_label)\n",
    "eval_dataset = eval_dataset.map(encode_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            # Find the last non-padding token\n",
    "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
    "            # Set all labels to ignore_index except for the last token\n",
    "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
    "            # If the last token in the text is, for example, \"2\", then this was processed with the old tokenizer into number_token_ids[2]\n",
    "            # But we don't actually want this because number_token_ids[2] could be something like 27, which is now undefined in the new lm_head. So we map it to the new lm_head index.\n",
    "            # if this line gives you a keyerror then increase max_seq_length\n",
    "            batch[\"labels\"][i, last_token_idx] = reverse_map[ batch[\"labels\"][i, last_token_idx].item() ]\n",
    "\n",
    "\n",
    "        return batch\n",
    "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1,\n",
    "    packing = False,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine_with_restarts\",\n",
    "        seed = 0,\n",
    "        output_dir = \"sft-qwen3-4b\",\n",
    "        num_train_epochs = 3,\n",
    "        report_to = \"wandb\",\n",
    "        group_by_length = True,\n",
    "\n",
    "        logging_strategy = \"epoch\",\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "    ),\n",
    "    data_collator = collator,\n",
    "    dataset_text_field = \"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8084c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current (trimmed) lm_head and bias\n",
    "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
    "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
    "\n",
    "# Create a new lm_head with shape [old_size, hidden_dim]\n",
    "hidden_dim = trimmed_lm_head.shape[1]\n",
    "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
    "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
    "\n",
    "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
    "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
    "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
    "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
    "\n",
    "# Update the model's lm_head weight and bias\n",
    "with torch.no_grad():\n",
    "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
    "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
    "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
    "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
    "\n",
    "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaba936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Create mapping dictionaries ---\n",
    "# Map from string labels (original data) to numeric IDs (for computation)\n",
    "label_to_id = {\n",
    "    'no': 0,\n",
    "    'intrinsic': 1,\n",
    "    'extrinsic': 2\n",
    "}\n",
    "# Reverse mapping from numeric IDs (predictions) to string labels (for display)\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "\n",
    "# --- Step 2: Prepare data and prompt ---\n",
    "# Build inference prompt template, removing the label part\n",
    "# Use rsplit to safely split from the last occurrence\n",
    "inference_prompt_template = prompt.rsplit(\"class {label}\", 1)[0] + \"class \"\n",
    "# inference_prompt_template = prompt.rsplit(\"lớp {label}\", 1)[0] + \"lớp \"\n",
    "\n",
    "# Re-create the \"text\" column for eval_df if it does not exist\n",
    "eval_df[\"text\"] = formatting_prompts_func(eval_df, inference_prompt_template)\n",
    "\n",
    "# Sort validation set by token length for efficient batching\n",
    "eval_df['token_length'] = eval_df['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
    "val_df_sorted = eval_df.sort_values(by='token_length').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Step 3: Initialize variables for evaluation ---\n",
    "display = 20\n",
    "batch_size = 1 # Can increase to 8 or 16 if GPU VRAM allows\n",
    "device = model.device\n",
    "correct_predictions = 0\n",
    "results = []\n",
    "\n",
    "predicts = []\n",
    "gt = []\n",
    "\n",
    "# --- Step 4: Evaluation loop ---\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(0, len(val_df_sorted), batch_size), desc=\"Evaluating\"):\n",
    "        batch_df = val_df_sorted.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Build prompts for the current batch\n",
    "        prompts = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            prompts.append(\n",
    "                inference_prompt_template.format(\n",
    "                    context=row[\"context\"],\n",
    "                    question=row[\"prompt\"],\n",
    "                    response=row[\"response\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Tokenize and move inputs to GPU\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(device)\n",
    "        \n",
    "        # Forward pass: get logits from model\n",
    "        logits = model(**inputs).logits\n",
    "        \n",
    "        # Extract logits of the last token in each sequence\n",
    "        last_idxs = inputs.attention_mask.sum(1) - 1\n",
    "        last_logits = logits[torch.arange(len(batch_df)), last_idxs, :]\n",
    "        \n",
    "        # Compute probabilities for number tokens (\"0\", \"1\", \"2\")\n",
    "        probs_all = F.softmax(last_logits, dim=-1)\n",
    "        probs = probs_all[:, number_token_ids]\n",
    "        \n",
    "        # Predicted IDs: argmax over probabilities\n",
    "        predicted_ids = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # True labels (convert from string to numeric IDs)\n",
    "        true_labels_str = batch_df['label'].tolist()\n",
    "        true_labels_ids = [label_to_id[label] for label in true_labels_str]\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct_predictions += sum([p == t for p, t in zip(predicted_ids, true_labels_ids)])\n",
    "\n",
    "        # Save results for inspection\n",
    "        for j in range(len(batch_df)):\n",
    "            pred_id = predicted_ids[j]\n",
    "            true_id = true_labels_ids[j]\n",
    "\n",
    "            predicts.append(pred_id)\n",
    "            gt.append(true_id)\n",
    "            \n",
    "            results.append({\n",
    "                \"id\": batch_df[\"id\"].iloc[j],\n",
    "                \"text\": batch_df['text'].iloc[j],\n",
    "                \"true_label\": id_to_label.get(true_id, \"N/A\"),\n",
    "                \"pred_label\": id_to_label.get(pred_id, \"N/A\"),\n",
    "                \"probs\": probs[j].float().cpu().numpy(),\n",
    "                \"is_correct\": pred_id == true_id\n",
    "            })\n",
    "\n",
    "# --- Step 5: Final evaluation results ---\n",
    "accuracy = 100 * correct_predictions / len(val_df_sorted)\n",
    "print(f\"\\nValidation Accuracy: {accuracy:.2f}% ({correct_predictions}/{len(val_df_sorted)})\")\n",
    "\n",
    "print(\"\\n--- Random Samples ---\")\n",
    "# Prioritize wrong samples for debugging\n",
    "wrong_samples = [s for s in results if not s['is_correct']]\n",
    "correct_samples = [s for s in results if s['is_correct']]\n",
    "\n",
    "samples_to_show = wrong_samples + correct_samples\n",
    "\n",
    "# Show up to `display` samples\n",
    "for s in random.sample(samples_to_show, min(display, len(results))):\n",
    "    status_icon = '✅' if s['is_correct'] else '❌'\n",
    "    print(f\"\\nText: {s['text']}\")\n",
    "    print(f\"True: {s['true_label']:<10} | Pred: {s['pred_label']:<10} {status_icon}\")\n",
    "    # Show probability distribution for each class\n",
    "    prob_str = \", \".join([f\"{id_to_label[k]}: {v:.3f}\" for k, v in enumerate(s['probs'], start=0)])\n",
    "    print(f\"Probs: [{prob_str}]\")\n",
    "    \n",
    "# Save wrong predictions to file\n",
    "with open(\"wrong_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in wrong_samples:\n",
    "        f.write(\"============================================\\n\")\n",
    "        f.write(f\"Text: {s['text']}\\n\")\n",
    "        f.write(f\"True: {s['true_label']} | Pred: {s['pred_label']}\\n\")\n",
    "        prob_str = \", \".join([f\"{id_to_label[k]}: {v:.3f}\" for k, v in enumerate(s['probs'], start=0)])\n",
    "        f.write(f\"Probs: [{prob_str}]\\n\\n\")\n",
    "\n",
    "print(f\"\\n❌ Saved {len(wrong_samples)} wrong samples into 'wrong_predictions.txt'\")\n",
    "\n",
    "\n",
    "# Save correct predictions to CSV\n",
    "correct_df = pd.DataFrame(correct_samples)\n",
    "correct_df.to_csv(\"correct_predictions.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Saved {len(correct_samples)} correct samples into 'correct_predictions.csv'\")\n",
    "\n",
    "# Clean up\n",
    "if 'token_length' in eval_df:\n",
    "    del eval_df['token_length']\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
