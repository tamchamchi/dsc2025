from unsloth import FastLanguageModel
from peft import PeftModel
import torch
import pandas as pd
import torch.nn.functional as F
from tqdm.auto import tqdm

max_seq_length = 1024*3
load_in_4bit=False
NUM_CLASSES = 3

prompt = """### INSTRUCTION ###
You are a fact-checking system.  
Classify the "Response" to the "Question" into one of three classes:  
- class 0: no ‚Üí Correct answer, consistent with context, no extra info.  
- class 1: intrinsic ‚Üí Contradicts or distorts context info.  
- class 2: extrinsic ‚Üí Adds info not in context.  
### INPUT ###
Context: {context}  
Question: {question}  
Response: {response}  
### OUTPUT ###
The correct answer is: class {label}"""

label_to_id = {
    'no': 0,
    'intrinsic': 1,
    'extrinsic': 2
}

id_to_label = {v: k for k, v in label_to_id.items()}

def init_model(model_name, checkpoint):

    model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = f"unsloth/{model_name}",
    load_in_4bit = load_in_4bit,
    max_seq_length = max_seq_length,
    )

    number_token_ids = []
    for i in range(0, NUM_CLASSES):
        number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])
    # keep only the number tokens from lm_head
    par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])

    old_shape = model.lm_head.weight.shape
    old_size = old_shape[0]

    model.lm_head.weight = par

    model = PeftModel.from_pretrained(model, checkpoint)

    # Save the current (trimmed) lm_head and bias
    trimmed_lm_head = model.lm_head.weight.data.clone()
    trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, "bias") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)

    # Create a new lm_head with shape [old_size, hidden_dim]
    hidden_dim = trimmed_lm_head.shape[1]
    new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)
    new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)

    # Fill in the weights and bias for the allowed tokens (number_token_ids)
    for new_idx, orig_token_id in enumerate(number_token_ids):
        new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]
        new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]

    # Update the model's lm_head weight and bias
    with torch.no_grad():
        new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)
        new_lm_head_module.weight.data.copy_(new_lm_head)
        new_lm_head_module.bias.data.copy_(new_lm_head_bias)
        model.lm_head.modules_to_save["default"] = new_lm_head_module

    print(f"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}")

    return model, tokenizer, number_token_ids

def formatting_prompts_func(dataset_, prompt_template):
    texts = []
    for index, row in dataset_.iterrows():
        # L·∫•y d·ªØ li·ªáu v√† strip() ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        context_ = str(row.get('context', '')).strip()
        question_ = str(row.get('prompt', '')).strip()
        response_ = str(row.get('response', '')).strip()
        
        # L·∫•y nh√£n ch·ªØ v√† lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        label_text = str(row.get('label', '')).strip()
        
        # *** D√íNG QUAN TR·ªåNG NH·∫§T: √Ånh x·∫° nh√£n ch·ªØ sang nh√£n s·ªë ***
        label_ = label_to_id.get(label_text, '') # D√πng .get ƒë·ªÉ tr√°nh l·ªói n·∫øu c√≥ nh√£n l·∫°
        
        # ƒêi·ªÅn th√¥ng tin v√†o khu√¥n m·∫´u
        text = prompt_template.format(
            context=context_, 
            question=question_, 
            response=response_, 
            label=label_
        )
        
        texts.append(text)
        
    return texts


def predict(model, tokenizer, number_token_ids, task):

    diff = pd.read_csv("./data/diff_results.csv")
    conditions = [
        ((diff['predict_label_file1'] == "no") & (diff['predict_label_file2'] == "intrinsic")) |
        ((diff['predict_label_file1'] == "intrinsic") & (diff['predict_label_file2'] == "no")),

        ((diff['predict_label_file1'] == "no") & (diff['predict_label_file2'] == "extrinsic")) |
        ((diff['predict_label_file1'] == "extrinsic") & (diff['predict_label_file2'] == "no")),

        ((diff['predict_label_file1'] == "intrinsic") & (diff['predict_label_file2'] == "extrinsic")) |
        ((diff['predict_label_file1'] == "extrinsic") & (diff['predict_label_file2'] == "intrinsic")),
    ]
    values = ["01", "02", "12"]

    diff['task'] = pd.Series(pd.NA, index=diff.index) 
    diff.loc[conditions[0], 'task'] = values[0]
    diff.loc[conditions[1], 'task'] = values[1]
    diff.loc[conditions[2], 'task'] = values[2]

    diff = diff[diff["task"]==task]
    
    private_df = pd.read_csv("./data/vihallu-private-test.csv")
    task_df = private_df[private_df['id'].isin(diff["id"])]

    output_file_path = f'./data/submit_{task}.csv'

    id_to_label = {
        0: 'no',
        1: 'intrinsic',
        2: 'extrinsic'
    }

    # S·ª≠ d·ª•ng l·∫°i prompt template cho inference
    # ƒê·∫£m b·∫£o bi·∫øn 'prompt' t·ª´ c√°c cell tr∆∞·ªõc v·∫´n t·ªìn t·∫°i
    inference_prompt_template = prompt.rsplit("class {label}", 1)[0] + "class "
    # inference_prompt_template = prompt.rsplit("l·ªõp {label}", 1)[0] + "l·ªõp "

    # --- B∆∞·ªõc 2: Ch·∫°y d·ª± ƒëo√°n tr√™n t·∫≠p test ---

    # Kh·ªüi t·∫°o c√°c bi·∫øn
    predictions = []
    batch_size = 1  # C√≥ th·ªÉ tƒÉng batch size ƒë·ªÉ inference nhanh h∆°n
    device = model.device

    # ƒê·∫∑t m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô ƒë√°nh gi√° (quan tr·ªçng!)
    model.eval()

    with torch.inference_mode():
        for i in tqdm(range(0, len(task_df), batch_size), desc="ƒêang d·ª± ƒëo√°n tr√™n file test"):
            batch_df = task_df.iloc[i:i+batch_size]
            
            # L·∫•y ID c·ªßa c√°c h√†ng trong batch
            batch_ids = batch_df['id'].tolist()
            
            # T·∫°o prompts cho batch hi·ªán t·∫°i
            prompts = []
            for _, row in batch_df.iterrows():
                prompts.append(
                    inference_prompt_template.format(
                        context=str(row.get('context', '')),
                        question=str(row.get('prompt', '')),
                        response=str(row.get('response', ''))
                    )
                )

            # Tokenize v√† ƒë∆∞a l√™n GPU
            inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=max_seq_length).to(device)
            
            # L·∫•y logits t·ª´ m√¥ h√¨nh
            logits = model(**inputs).logits
            
            # Ch·ªâ l·∫•y logits c·ªßa token cu·ªëi c√πng
            last_idxs = inputs.attention_mask.sum(1) - 1
            last_logits = logits[torch.arange(len(batch_df)), last_idxs, :]
            
            # L·∫•y x√°c su·∫•t ch·ªâ cho c√°c token s·ªë ("0", "1", "2", "3")
            probs = F.softmax(last_logits, dim=-1)[:, number_token_ids]
            
            # L·∫•y ID d·ª± ƒëo√°n (0, 1, 2, ho·∫∑c 3)
            predicted_ids = torch.argmax(probs, dim=-1).cpu().numpy()
            
            # Chuy·ªÉn ƒë·ªïi ID d·ª± ƒëo√°n sang nh√£n ch·ªØ v√† l∆∞u k·∫øt qu·∫£
            for j in range(len(batch_df)):
                pred_id = predicted_ids[j]
                pred_label = id_to_label.get(pred_id, "N/A") # D√πng .get ƒë·ªÉ an to√†n
                
                predictions.append({
                    'id': batch_ids[j],
                    'predict_label': pred_label
                })

        # --- B∆∞·ªõc 3: T·∫°o v√† l∆∞u file submission ---

        # Chuy·ªÉn danh s√°ch k·∫øt qu·∫£ th√†nh DataFrame
        submission_df = pd.DataFrame(predictions)

        # L∆∞u DataFrame ra file CSV, kh√¥ng bao g·ªìm c·ªôt index
        submission_df.to_csv(output_file_path, index=False)

        print(f"\n‚úÖ ƒê√£ t·∫°o file submission th√†nh c√¥ng t·∫°i: '{output_file_path}'")
        print("Xem tr∆∞·ªõc 5 d√≤ng ƒë·∫ßu c·ªßa file submission:")
        print(submission_df.head())


if __name__ == "__main__":
    import argparse
    import os

    parser = argparse.ArgumentParser(description="Fact-checking inference script with task selection")

    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen2.5-7B-Instruct",   # üëà m·∫∑c ƒë·ªãnh
        help="T√™n m√¥ h√¨nh HuggingFace (m·∫∑c ƒë·ªãnh: Qwen2.5-7B-Instruct)"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="./checkpoint/Qwen2.5-7B-Instruct",  # üëà m·∫∑c ƒë·ªãnh
        help="ƒê∆∞·ªùng d·∫´n checkpoint ƒë√£ fine-tune (m·∫∑c ƒë·ªãnh: ./checkpoint/Qwen2.5-7B-Instruct)"
    )
    parser.add_argument(
        "--task",
        type=str,
        choices=["01", "02", "12"],
        default="01",  # üëà m·∫∑c ƒë·ªãnh
        help="Ch·ªçn task c·∫ßn ch·∫°y: 01, 02 ho·∫∑c 12 (m·∫∑c ƒë·ªãnh: 01)"
    )

    args = parser.parse_args()

    # Load model
    model, tokenizer, number_token_ids = init_model(args.model_name, args.checkpoint)

    # G·ªçi h√†m predict v·ªõi task t·ª´ args
    predict(model, tokenizer, number_token_ids, args.task)